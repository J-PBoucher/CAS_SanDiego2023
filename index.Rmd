---
title: "Bonus-Malus Scales Models for Predictive Modeling"
subtitle: "CAS Ratemaking, Product and Modeling Seminar"
author: "Jean-Philippe Boucher"
institute: "Université du Québec à Montréal"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    seal: false
    css: "theme_cara.css"
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r include=FALSE, eval=FALSE}

xaringan_to_pdf('index.html', NULL, 1, FALSE)

```


```{r include = F}
library(tidyverse)
library(xaringan)
library(xaringanthemer)
library(kableExtra)
library(DT)
library(dplyr)
library(ggplot2)
library(kableExtra)
library(scales)
library(MASS)
library(gamlss)

load('Data/df2.Rda')


```

class: title-slide
background-image: url(images/logo_chaire.jpg), url(images/background.jpg)
background-size: 25%, cover
background-position: 98% 98%, center

.titre-page-titre[Bonus-Malus Scales Models for Predictive Modeling]
<br />
.sous-titre-page-titre[CAS Ratemaking, Product and Modeling Seminar]
<br />
<br />
***
<br />
<br />
.sous-sous-titre-page-titre[.mon-style-bleu[] Prof. Jean-Philippe Boucher, ACIA, Ph.D. <br /> .mon-style-bleu[] Co-operators Chair in Actuarial Risk Analysis <br /> Université du Québec à Montréal (UQAM) <br /> .mon-style-bleu[] March 15th, 2023]



---

background-image: url("images/GitHub.png")
background-position: 80% 50%
background-size: 40%

#  Scripts and dataframe available on Github 

.pull-left[

On my <a href="https://github.com/J-PBoucher/CAS_SanDiego2023">github page</a>, you can find:
- this presentation; 
- all R script codes used;
- the dataframe *df2.Rda*.

<center><a href="https://github.com/J-PBoucher">https://github.com/J-PBoucher</a></center>
]

---

# R Preamble (to replicate the results)

```{r include = T, eval=F}
library(tidyverse)
library(xaringan)
library(xaringanthemer)
library(kableExtra)
library(DT)
library(dplyr)
library(ggplot2)
library(kableExtra)
library(scales)
library(MASS)
library(gamlss)

load('Data/df2.Rda')


```

---


# Summary of the presentation

## Part I - Ratemaking with Cross-Section Data  
- Basic Count Distributions;
- Credibility Models and Predictive Ratemaking;
- Bonus-Malus Scales Models.

## Part II - Ratemaking with Panel Data  
- Families of Count Distributions;
- Observed Predictive Premiums;
- Bonus-Malus Scales Models Revisited.

## Part III - Actual Challenges
- Entry levels and new insureds;
- Penalties and *a priori* risks.

---

background-image: url("images/Paper1.png")
background-position: 80% 50%
background-size: 40%

# Reference 

.pull-left[
### The first part of the presentation is based on Sections 1-3 of: 

J.-P. Boucher (2022). Bonus-Malus Scale Models: Creating Artificial Past Claims History. *Annals of Actuarial Science*, 1-27. ]


---

# Data used 

## Farm insurance database (in the published paper)

- We used farm insurance data from a major insurance company in Canada; 
- We were able to use contracts from 2014 to 2019;
- Past claims from 1999 to 2014 were available.

## Farm data cannot be shared...

- Instead, we illustrate our models with fictive car insurance data.
- Using the dataframe *df2.Rda*;
- Possible to replicate the results of this presentation;
- Available on my *github* page (reference at the end).

---

class: inverse

## .rougeb[Part I - Ratemaking with Cross-Section Data]    

- Basic count distributions  
- Credibility Models and Predictive Ratemaking  
- Bonus-Malus Scales Models  

## Part II - Ratemaking with Panel Data  

- Families of Count Distributions  
- Observed Predictive Premiums  
- Bonus-Malus Scales Models Revisited

## Part III - Actual Challenges

- Entry levels and new insureds  
- Penalties and *a priori* risks

---

# Classic insurance database 

```{r, echo=FALSE, warning=FALSE}
tempo <- df2[sample(1:nrow(df2)), 1:24]
knitr::kable(head(tempo), format = "html", table.attr = "style='width:95%;'") %>% scroll_box(width = "100%")
```

---

# Summary of the dataset 

## Basic statistics

```{r, echo=FALSE, warning=FALSE}
res <- df2 %>%
  group_by(NbClaims) %>%
  summarize(expo = sum(risk_expo),
            nb = n()) 
res$pctexpo <- res$expo/sum(res$expo)
res$pctnb <- res$nb/sum(res$nb)
res$pctexpo <- percent(res$pctexpo, accuracy = 0.1)
res$pctnb <- percent(res$pctnb, accuracy = 0.1)

res <- res[,c(1,3,5,2,4)]
colnames(res)[1] <- 'Nb. of Claims'
colnames(res)[2] <- 'Nb. of obs.'
colnames(res)[3] <- '% of obs.'
colnames(res)[4] <- 'Total exposition'
colnames(res)[5] <- '% of exposition'

knitr::kable(res, format = "html", align = "ccccc",  digits = c(0,0,0, 0,0), table.attr = "style='width:95%;'", format.args = list(big.mark = ",")) 

mean <- mean(df2$NbClaims)
variance <- sd(df2$NbClaims)^2

test <- data.frame(c(mean, variance))

colnames(test)[1] <- ''
rownames(test)[1] <- 'Mean'
rownames(test)[2] <- 'Variance'

knitr::kable(test, format = "html", digits = c(3), table.attr = "style='width:95%;'") 
```

---

# Available covariates

Many fictional covariates are available in the dataframe. For illustration, we will however only focus on 4 (fictional) covariates to model the number of claims:

```{r, echo=FALSE, cache=FALSE, warning=FALSE}
test <- data.frame(colnames(df2[,c(18,14,12,20)]))
list <- c(18,14,12,20)
test$value <- NA
for(i in 1:4){
  num <- list[i]
  crossdata <- df2[order(df2[,num]),]
  test[i,2] <- paste(c(unique(crossdata[,num])))
}
test$value <- sub('c','',test$value)

colnames(test)[1] <- 'Column'
colnames(test)[2] <- 'Values'
knitr::kable(test, format = "html", row.names = F, table.attr = "style='width:95%;'") 
```

---

# Basis of covariates selection

## Possible techniques
- Minimum-bias techniques (...old);  
- Generalized linear models and GLM-net (Ridge and Lasso);  
- Random Forests;  
- Neural Networks;  
- etc.  

## Literature review (from actuarial sciences):
- Denuit, M., Hainaut, D. & Trufin, J. (2019), Springer Nature:  
  - *Effective statistical learning methods for actuaries I: GLMs and Extensions*,  
  - *Effective statistical learning methods for actuaries II: Tree-Based Methods and Extensions*,  
  - *Effective statistical learning methods for actuaries III: Neural Networks and Extensions*,  
- Wüthrich, M. V., & Merz, M. (2023). *Statistical foundations of actuarial learning and its applications*. 


---

# Prior ratemaking

## Poisson distribution

Commonly, the starting point for the modeling the number of claims is the Poisson distribution:

$$\Pr[N_{i}= n_{i}|\boldsymbol{X}_i] = \frac{\lambda_{i}^{n_{i}} e^{-\lambda_{i}}}{n_{i}!} \ 
\text{, with } \ \ \lambda_{i} = \exp(\boldsymbol{X}_{i}' \boldsymbol{\beta}).$$

With $E[N_i] = \lambda_i$, this form of ratemaking is usually called *a priori* ratemaking. In this framework, the actuary does not consider the past claim experience of the insureds.  

## Alternatives

To correct the equidispersion of the Poisson or other problems, the most popular alternatives to the Poisson are:

- Negative binomial (NB2 or NB1);
- Poisson-inverse gaussian (PIG2 or PIG1);
- Poisson-lognormal (PLN2 or PLN1);
- Zero-Inflated distributions.

---

# Predictive ratemaking

## Conditional expected value

The insurer is also interested in a premium that considers past contracts: 

$$E[N_{i,T}|n_{i,1},...,n_{i,T-1}, \boldsymbol{X}_{i, (1:T-1)}].$$ 

## Problem with cross-section data

- We suppose an independance between each line of the dataset;
- We do not directly observe the claim experience of an insured for his 2nd, 3rd, ..., contracts.

## Classic assumption (Bühlmann, 1967)

We suppose that each insured has his own random heterogeneity component (usually noted $\Theta$) that affects all his insurance contracts. 

---

# Gamma heterogeneity

If we suppose that $N_i|\Theta=\theta \sim Poisson(\lambda_i \theta)$, with $\Theta \sim gamma(\alpha, \tau = \alpha)$, we have:

$$\Pr[N_i=n] =  \int_0^{\infty} \frac{(\lambda_i \theta)^{n} e^{-\lambda_i \theta}}{n!} \frac{\alpha^{\alpha}}{\Gamma(\alpha)} \theta^{\alpha - 1} e^{-\alpha \theta} d\theta $$
## Negative binomial 2 distribution

It can be shown that the Poisson-gamma leads to the NB2, having the following probability function:

$$\Pr[N_i=n] = \binom{\alpha + n - 1}{n} \left(\frac{\alpha}{\lambda_i + \alpha}\right)^{\alpha} 
\left(\frac{\lambda_i}{\lambda_i + \alpha}\right)^{n}$$

## Moments

The NB2 has and expected value of $E[N_i] = \lambda_i$ and a variance $Var[N_i] = \lambda_i + \frac{\lambda_i^2}{\alpha}$, which means that the NB2 allows for overdispersion.

---

# Bayesian approach 

The addition of an heterogeneity term leads to the famous credibility models of Buhlmann or Buhlmann-Straub (exam C, or STAM).

## Predictive premium

It becomes possible to express the predictive premium based on the past number of claims $n$, and the past values of $\lambda$ : 

$$E[N_{i,T}|n_{i,1},..., n_{i,T-1}, \boldsymbol{X}_{i,T}] = \lambda_{i,T} \frac{\alpha + \sum_{t=1}^{T-1} n_{i,t}}{\alpha + \sum_{t=1}^{T-1} \lambda_{i,t}}$$ 

Even if the actuary cannot directly observe the average predictive value with the data, he is able to compute predictive premiums at the cost of making the constact random effect assumption.  

---

# R scripts: Poisson GLM

## Split data 

```{r, eval=TRUE}

db.train <- df2 %>% filter(Type=='TRAIN')
db.test <- df2 %>% filter(Type=='TEST')

```

## Poisson GLM 

```{r, eval=TRUE}
score.nbclaim <- as.formula(NbClaims ~ car_color + need_glasses + territory + language + food + offset(log(risk_expo)))
Poisson   <- glm(score.nbclaim, family=poisson(link=log), data=db.train)
```

---

# R scripts: Negative Binomial 2

## Packages to use 

You can directly estimate the parameters by maximum likelihood by maximising the log-probability, or you can use R packages.  The *MASS* package (or the *gamlss*, for example) can be used to estimate the parameters of a NB2 distribution:

```{r, eval=FALSE}
library(MASS)
```


```{r, eval=FALSE}
nb2.MASS <- glm.nb(score.nbclaim, data=db.train) 
```

```{r include = F}
nb2.MASS <- glm.nb(score.nbclaim, data=db.train) 
```


---

# Results

## Comparison

```{r, echo=FALSE, warning=FALSE, message=FALSE}
parm <- Poisson$coefficients
MASS.parm <- c(nb2.MASS$coefficients, nb2.MASS$theta)
parm[9] <- NA
table <- cbind(parm, MASS.parm)
colnames(table)[1] <- 'Poisson'
colnames(table)[2] <- 'NB2'
rownames(table)[9] <- "$\\alpha$"
knitr::kable(table, format = "html", align = "ccccc",  digits = c(4,4), table.attr = "style='width:95%;'", format.args = list(big.mark = ",")) 

```

---

# Prediction quality on the test set

Even if it is not the objective of this presentation, we can compare the fit and the prediction quality of the Poisson and the Negative Binomial 2.  For the training set, the loglikelihood is used and a logarithmic score (LS) is used on the test set:

$$LS = - \sum_{i=1}^m \log(\Pr(N_i=n_i|\boldsymbol{X}_i))$$

## Computation

```{r, echo=TRUE, warning=FALSE, message=FALSE}
db.test$pred <- predict(Poisson, newdata=db.test, type="response") 
logs.Poisson <- -sum(dpois(db.test$NbClaims, db.test$pred, log=TRUE))
ll.Poisson <- logLik(Poisson)

db.test$pred <- predict(nb2.MASS, newdata=db.test, type="response") 
alpha <- 1/nb2.MASS$theta
tau <- 1/nb2.MASS$theta
ll <- lgamma(db.test$NbClaims + alpha) - lgamma(alpha) - lgamma(db.test$NbClaims+1) + alpha*log(tau) - alpha*log(db.test$pred+tau) + db.test$NbClaims*log(db.test$pred) - db.test$NbClaims*log(db.test$pred+tau)
logs.NB2 <- -sum(ll)
ll.NB2 <- logLik(nb2.MASS)

```

---

# Comparison of models

## Results

A correction for the training dataset, such as the AIC/BIC, must be applied for each model because they do not have he same number of parameters.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
table<- data.frame(cbind(c(ll.Poisson,ll.NB2), c(logs.Poisson,logs.NB2)))

colnames(table)[1] <- 'Log-likelihood (Train)'
colnames(table)[2] <- 'Logarithmic Score (Test)'
rownames(table)[1] <- 'Poisson'
rownames(table)[2] <- 'Negative Binomial 2'

knitr::kable(table, format = "html", align = "ccccc",  digits = c(4,4), table.attr = "style='width:95%;'", format.args = list(big.mark = ",") ) 

```

---

# Predictive ratemaking

## Distribution of the heterogeneity

Based on the bayesian model, by fitting a Negative Binomial distribution (NB2) on claim counts data, we know that the heterogeneity of our portfolio $\Theta$ is following a gamma( $\alpha = 2.1244$, $\tau = \alpha = 2.1244$).

## Predictive premiums

The predictive premium of an insured with $n_{i, \bullet} = \sum_{t=1}^{T-1} n_{i,t}$ past claims, and 
$\lambda_{i, \bullet} = \sum_{t=1}^{T-1} \lambda_{i,t}$ as the sum of past *a priori* premiums, is equal to:

$$E[N_{i,T}|n_{i,1},..., n_{i,T-1}, \boldsymbol{X}_{i,T}] = \lambda_{i,T} \frac{\alpha + \sum_{t=1}^{T-1} n_{i,t}}{\alpha + \sum_{t=1}^{T-1} \lambda_{i,t}} = \lambda_{i,T} \frac{2.1244 + n_{i, \bullet}}{2.1244 +  \lambda_{i,\bullet}}$$ 

In the STAM exam, we were able to analyse in details this equation.

---

# Practical use

However, even if the Poisson-gamma model is theoretically correct, and even studied in the preliminary exams, this predictive rating approach is almost never used in practice:

- There is not weight in $\sum_{t=1}^{T-1} n_{i,t}$.  
  + That means that a claim from 10 or 20 years ago will have the same impact of the premium that an accident that was claimed last year;  
  
- The value of $\sum_{t=1}^{T-1} \lambda_{i,t}$ depend on the estimated values of $\beta$, and should then be computed each year.  
  + That means that insurers should keep all past covariates $\boldsymbol{X}_{i,t}$, from $t=1, \ldots, T$ of all their insureds.  For new insureds, this is even more complicated.

---

# Bonus-malus scales

To compute predictive premiums, actuaries have created **Bonus-Malus Scales** (BMS) models.  BMS are class systems where the insured's level $\ell$ increases or decreases only by the number of claims.

### Structure of the BMS (example with 6 levels)

```{r, echo=FALSE, warning=FALSE, message=FALSE}
table<- data.frame(cbind(c('1', '2', '3', '4', '5', '6'), c(0.879, 0.986, 1.017, 1.141, 1.197, 1.296)))

colnames(table)[1] <- 'Level'
colnames(table)[2] <- 'Relativities'

knitr::kable(table, format = "html", align = "cc", row.names = F,  digits = c(4,4), table.attr = "style='width:95%;'", format.args = list(big.mark = ",")) 

```

---

# Transition rules

A BMS is defined by its number of levels and by its transition rules.  If we suppose a BMS with $6$ levels (from 1 to 6), we can than create a BMS having the following rules:

- A new insured has an entry level $1$;    
- The BMS level of an insured without claim will be lowered by 1 (-1);    
- The BMS level of an insured will increase by the number of claims times 2 (+2).   

This is called a BMS -1/+2, and its transition rule can be summarized as:

```{r, echo=FALSE, warning=FALSE, message=FALSE}
start <- c(1,2,3,4,5,6)
s0 <- c(1,1,2,3,4,5)
s1 <- c(3,3,4,5,6,6)
s2 <- c(4,5,6,6,6,6)
s3 <- c(6,6,6,6,6,6)

table<- data.frame(cbind(start, s0, s1, s2, s3)) 

colnames(table)[1] <- 'Starting Level (time t)'
colnames(table)[2] <- 'x=0'
colnames(table)[3] <- 'x=1'
colnames(table)[4] <- 'x=2'
colnames(table)[5] <- 'x>3'

knitr::kable(table, format = "html", align = "ccccc", row.names = F,  digits = c(4,4), table.attr = "style='width:95%;'", format.args = list(big.mark = ",")) %>%
  add_header_above(c(" ", "Level at time t+1, if x claims" = 4))

```

---


# Transition matrix

This means that for a specific distribution, for example a Poisson or a NB2 distribution, it becomes possible to construct the transition matrix from time $t$ to time $t+1$.

```{r, echo=FALSE, warning=FALSE, message=FALSE}

s1 <- c('1','Pr(N=0)','0', 'Pr(N=1)','0', 'Pr(N=2)', 'Pr(N>2)')
s2 <- c('2','Pr(N=0)','0', '0','Pr(N=1)', '0', 'Pr(N>1)')
s3 <- c('3','0','Pr(N=0)', '0','0', 'Pr(N=1)', 'Pr(N>1)')
s4 <- c('4','0','0', 'Pr(N=0)','0', '0', 'Pr(N>0)')
s5 <- c('5','0','0', '0','Pr(N=0)', '0', 'Pr(N>0)')
s6 <- c('6','0','0', '0','0', 'Pr(N=0)', 'Pr(N>0)')

table<- data.frame(rbind(s1, s2, s3, s4, s5, s6)) 

colnames(table)[1] <- 'Level at time t'
colnames(table)[2] <- '1'
colnames(table)[3] <- '2'
colnames(table)[4] <- '3'
colnames(table)[5] <- '4'
colnames(table)[6] <- '5'
colnames(table)[7] <- '6'

knitr::kable(table, format = "html", align = "ccccc", row.names=F, table.attr = "style='width:95%;'", format.args = list(big.mark = ",")) %>%
  add_header_above(c(" ", "Level at time t+1" = 6))

```

---

# Calibration of the BMS

## Covariates and risk characteristics

We can use the dataset of this example.  We suppose an insured with some specific risk characteristics, for example, the insured might drive a black car, be an English speaker, be vegan, etc.  

We will suppose a Poisson distribution, this insured has the following *a priori* premium: $\lambda = 0.2532$.

## Heterogeneity

For simplicity, we will still suppose $N_i|\Theta=\theta \sim Poisson(\lambda_i \theta)$, but $\Theta$ will represent two kinds of drivers:

```{r, echo=FALSE, warning=FALSE, message=FALSE}
s0 <- c(0.667, 0.333)
s1 <- c(0.750, 1.500)
s2 <- c(0.1899, 0.3798)

table<- data.frame(cbind(s0, s1, s2)) 
table[,1] <- percent(table[,1], accuracy = 0.1)

rownames(table)[1] <- 'Good driver'
rownames(table)[2] <- 'Bad driver'


knitr::kable(table, format = "html", col.names = c("Proportion", "$\\theta$", "$\\lambda \\theta$"), 
             align = "ccccc", row.names = T,  digits = c(3,3, 4), table.attr = "style='width:95%;'") 


```


---

# Transition matrix for each type of drivers


```{r, echo=FALSE, warning=FALSE, message=FALSE}

lambda <- 0.1899
p0 <- exp(-lambda)
p1 <- lambda*exp(-lambda)
p2 <- (lambda^2)*exp(-lambda)/2
s1 <- c(p0,0,p1,0,p2,1-(p0+p1+p2))
s2 <- c(p0,0,0,p1,0,1-p1-p0)
s3 <- c(0,p0,0,0,p1,1-p1-p0)
s4 <- c(0,0,p0,0,0,1-p0)
s5 <- c(0,0,0,p0, 0, 1-p0)
s6 <- c(0,0,0,0,p0,1-p0)
matrix1 <- rbind(s1, s2, s3, s4, s5, s6)

lambda <- 0.3798
p0 <- exp(-lambda)
p1 <- lambda*exp(-lambda)
p2 <- (lambda^2)*exp(-lambda)/2
s1 <- c(p0,0,p1,0,p2,1-(p0+p1+p2))
s2 <- c(p0,0,0,p1,0,1-p1-p0)
s3 <- c(0,p0,0,0,p1,1-p1-p0)
s4 <- c(0,0,p0,0,0,1-p0)
s5 <- c(0,0,0,p0, 0, 1-p0)
s6 <- c(0,0,0,0,p0,1-p0)
matrix2 <- rbind(s1, s2, s3, s4, s5, s6)

table1<- data.frame(matrix1)
table1[,1] <- percent(table1[,1], accuracy = 0.1)
table1[,2] <- percent(table1[,2], accuracy = 0.1)
table1[,3] <- percent(table1[,3], accuracy = 0.1)
table1[,4] <- percent(table1[,4], accuracy = 0.1)
table1[,5] <- percent(table1[,5], accuracy = 0.1)
table1[,6] <- percent(table1[,6], accuracy = 0.1)

colnames(table1)[1] <- ''
colnames(table1)[2] <- ''
colnames(table1)[3] <- ''
colnames(table1)[4] <- ''
colnames(table1)[5] <- ''
colnames(table1)[6] <- ''

table2<- data.frame(matrix2)
table2[,1] <- percent(table2[,1], accuracy = 0.1)
table2[,2] <- percent(table2[,2], accuracy = 0.1)
table2[,3] <- percent(table2[,3], accuracy = 0.1)
table2[,4] <- percent(table2[,4], accuracy = 0.1)
table2[,5] <- percent(table2[,5], accuracy = 0.1)
table2[,6] <- percent(table2[,6], accuracy = 0.1)

colnames(table2)[1] <- ''
colnames(table2)[2] <- ''
colnames(table2)[3] <- ''
colnames(table2)[4] <- ''
colnames(table2)[5] <- ''
colnames(table2)[6] <- ''

```


With the mean parameter of the Poisson distribution, it becomes possible to compute the transition matrix for both insured:

.pull-left[

### Good driver (0.1899)

```{r, echo=FALSE, warning=FALSE, message=FALSE}
knitr::kable(table1, format = "html", align = "ccccc", row.names=F, table.attr = "style='width:95%;'", format.args = list(big.mark = ",")) 

```

]

.pull-right[

### Bad driver (0.3798)

```{r, echo=FALSE, warning=FALSE, message=FALSE}
knitr::kable(table2, format = "html", align = "ccccc", row.names=F, table.attr = "style='width:95%;'", format.args = list(big.mark = ",")) 

```

]

---

# Stationnary matrix for each type of drivers

The long-term distribution of insured within the levels of the BMS can also be computed, where the initial BMS level at time $t=1$ does not have any impact.

```{r, echo=FALSE, warning=FALSE, message=FALSE}

for(i in 1:30){
  matrix1 <- matrix1%*%matrix1
  matrix2 <- matrix2%*%matrix2
}

table1<- data.frame(matrix1)
table1[,1] <- percent(table1[,1], accuracy = 0.1)
table1[,2] <- percent(table1[,2], accuracy = 0.1)
table1[,3] <- percent(table1[,3], accuracy = 0.1)
table1[,4] <- percent(table1[,4], accuracy = 0.1)
table1[,5] <- percent(table1[,5], accuracy = 0.1)
table1[,6] <- percent(table1[,6], accuracy = 0.1)

colnames(table1)[1] <- ''
colnames(table1)[2] <- ''
colnames(table1)[3] <- ''
colnames(table1)[4] <- ''
colnames(table1)[5] <- ''
colnames(table1)[6] <- ''

table2<- data.frame(matrix2)
table2[,1] <- percent(table2[,1], accuracy = 0.1)
table2[,2] <- percent(table2[,2], accuracy = 0.1)
table2[,3] <- percent(table2[,3], accuracy = 0.1)
table2[,4] <- percent(table2[,4], accuracy = 0.1)
table2[,5] <- percent(table2[,5], accuracy = 0.1)
table2[,6] <- percent(table2[,6], accuracy = 0.1)

colnames(table2)[1] <- ''
colnames(table2)[2] <- ''
colnames(table2)[3] <- ''
colnames(table2)[4] <- ''
colnames(table2)[5] <- ''
colnames(table2)[6] <- ''


```


.pull-left[

### Good driver (0.1899)

```{r, echo=FALSE, warning=FALSE, message=FALSE}
knitr::kable(table1, format = "html", align = "ccccc", row.names=F, table.attr = "style='width:95%;'", format.args = list(big.mark = ",")) 

```

]

.pull-right[

### Bad driver (0.3798)

```{r, echo=FALSE, warning=FALSE, message=FALSE}
knitr::kable(table2, format = "html", align = "ccccc", row.names=F, table.attr = "style='width:95%;'", format.args = list(big.mark = ",")) 

```

]


---

# BMS Relativities

## Posterior distribution

Knowing the *a priori* distribution of the heterogeneity $\Theta$, and the stationary distribution within the levels of the BMS for all types of drivers, we can compute the posterior distribution of $\Theta$, conditional on the level $\ell$:

\begin{align*}
\Pr[\text{Good driver}| L = \ell] &= \frac{\Pr[L= \ell|\text{Good driver}] \Pr[\text{Good driver}]}
{\Pr[L= \ell| \text{Good driver}] \Pr[\text{Good driver}] + \Pr[L= \ell| \text{Bad driver}] \Pr[\text{Bad driver}]}\\
&= 1 - \Pr[\text{Bad driver}| L = \ell]
\end{align*}

## Conditional expectation

With the posterior distribution of $\Theta$, the BMS relativity of each level can be computed with the conditional expectation:
 
$$r_{\ell} = E[\Theta| L = \ell] = 0.75 \times \Pr[\text{Good driver}| L = \ell] + 1.50 \times \Pr[\text{Bad driver}| L = \ell] $$
---

# Results

Simple calculations with our numerical example lead to:

```{r, echo=FALSE, warning=FALSE, message=FALSE}

prob1 <- matrix(nrow=1, ncol=ncol(table1))
for(i in 1:ncol(table1)){
  p1 <- matrix1[1,i]*0.667
  p2 <- matrix2[1,i]*0.333
  prob1[i] <- p1/(p1+p2)
}

mean <- prob1*0.75 + (1-prob1)*1.5
table1 <- data.frame(rbind(prob1, 1-prob1, mean))

table1[,1] <- percent(table1[,1], accuracy = 0.1)
table1[,2] <- percent(table1[,2], accuracy = 0.1)
table1[,3] <- percent(table1[,3], accuracy = 0.1)
table1[,4] <- percent(table1[,4], accuracy = 0.1)
table1[,5] <- percent(table1[,5], accuracy = 0.1)
table1[,6] <- percent(table1[,6], accuracy = 0.1)

colnames(table1)[1] <- '1'
colnames(table1)[2] <- '2'
colnames(table1)[3] <- '3'
colnames(table1)[4] <- '4'
colnames(table1)[5] <- '5'
colnames(table1)[6] <- '6'

rownames(table1)[1] <- 'Good driver'
rownames(table1)[2] <- 'Bad driver'
rownames(table1)[3] <- 'BMS relativity'

knitr::kable(table1, format = "html", align = "cccccc", row.names=T, table.attr = "style='width:95%;'", format.args = list(big.mark = ",")) %>%
  add_header_above(c(" ", "BMS Level" = 6))



```

---

# Computation of relativities

The results can be generalized with continuous heterogeneity, when $\Theta \sim gamma(\alpha, \alpha)$, for example.

In those cases, it can be shown that BMS relativities are computed using:

$$r_{\ell} = \frac{ \int_0^{\infty} \theta  \Pi_{\ell}(\lambda \theta) g(\theta) d\theta }
{\int_0^{\infty} \Pi_{\ell}(\lambda \theta) g(\theta) d\theta} \ \ \  \text{, for } \ell = 1,...,\ell_{max}.$$

where: 

- $g(\theta)$ is the prior heterogeneity distribution;   

- $\Pi_{\ell}(\lambda \theta)$ is the $\ell$ line component of $\Pi(\lambda \theta)$, the stationary distribution of insured of mean $\lambda \theta$.

---

# Summary of the BMS

## Characteristics of the BMS

To calculate the relativities, the actuary must select the characteristics of the BMS. Different choices will lead to different values of relativities $r_{\ell}, \ell = 1,...,\ell_{max}$:  
- The maximum number of levels $\ell_{max}$ of the BMS;    
- The value of the penalty for each claim, i.e. the penalty structure of the BMS (ex: -1/+2);  
- Other transition rules (for example: 3-5 years without claim automatically gives the largest discount);
- The entry level for new insureds.  

## Selection of the best BMS

A variety of methods has been developed in the scientific literature to select the best BMS:  
- The coefficient of variation;  
- The mean-square error of prediction;  
- The elasticity of the BMS.  

---

# For more details

.pull-left-50[
Denuit, M., Maréchal, X., Pitrebois, S., & Walhin, J. F. (2007). *Actuarial modelling of claim counts: Risk classification, credibility and bonus-malus systems*. John Wiley & Sons.  

<br />
```{r, echo = F, out.width = "40%", fig.align = "center"}
knitr::include_graphics("images/Denuit.png")
```
]

.pull-right-50[
Lemaire, J. (1995). *Bonus-malus systems in automobile insurance (Vol. 19)*. Springer science & business media.  

<br />
<br />
```{r, echo = F, out.width = "40%", fig.align = "center"}
knitr::include_graphics("images/Lemaire.png")
```

]

---

class: inverse

## Part I - Ratemaking with Cross-Section Data     

- Basic count distributions  
- Credibility Models and Predictive Ratemaking  
- Bonus-Malus Scales Models  

## .rougeb[Part II - Ratemaking with Panel Data]  

- Families of Count Distributions  
- Observed Predictive Premiums  
- Bonus-Malus Scales Models Revisited

## Part III - Actual Challenges

- Entry levels and new insureds  
- Penalties and *a priori* risks

---

# Actual insurance database 

Insureds (or even vehicles) are observed over time.  Here, the dataframe *df2.Rda* contains 25,078 vehicles, each observed for 5 years.

```{r, echo=FALSE, warning=FALSE}
knitr::kable(head(df2[,1:25]), format = "html", table.attr = "style='width:95%;'") %>% scroll_box(width = "100%")
```

---

background-image: url("images/Verbeke.png")
background-position: 80% 50%
background-size: 20%

# Claim count for panel data 

.pull-left[

### Families of models

We have to suppose a form of dependance between all contracts of the same insured/vehicle. This form of data is called panel data, or longitudinal data.

For count distributions, panel data modeling admits 3 different families (see Molenberghs & Verbeke, 2005):

- Transition models (for example: Time series for count data);    
- Marginal approach (for example: Generalized Estimating Equations - GEE);  
- Conditional approach with random effects.

]

---

# Conditional approach with random effects

## General form 

In actuarial science, the conditional approach is the most popular approach. It that can be seen as a generalization of the heterogeneity approach seen earlier:

$$\Pr[N_{i,1}=n_{i,1},...,N_{i,T}=n_{i,T}] = \int_{D_{\Theta}} \left(\prod_{t=1}^T \Pr[N_{i,t}=n_{i,t}|\theta] \right) g(\theta) d\theta.$$

## The Poisson-gamma model revisited

A conditional Poisson distribution, with gamma random effects leads to the multivariate negative binomial distribution (MVNB), a generalization of the NB2 distribution:

$$\Pr[N_{i,1}=n_{i,1},...,N_{i,T}=n_{i,T}] = \left(\prod_{t=1}^{T} \frac{(\lambda_{i,t})^{n_{i,t}}}{n_{i,t}!} \right)
\frac{\Gamma(\sum_{t=1}^T n_{i,t} + \alpha)}{\Gamma(\alpha)} \left(\frac{\alpha}{\sum_{i=1}^T \lambda_{i,t} + \alpha}\right)^{\alpha}
\left(\sum_{i=1}^T \lambda_{i,t} + \alpha\right)^{-\sum_{t=1}^T n_{i,t}}.$$

---

# R scripts: Multivariate Negative Binomial (MVNB)

## Packages to use 

To our knowledge, a R package to estimate the MVNB distribution does not exist. An old package *multinbmod* worked in the past but has not been updated recently.

Instead, a simpler solution is to simply write down the log-likelihood of the MVNB in a function (here *max.loglike.MVNB*) and use the *optim* or the *optimx* solver:

```{r, eval=FALSE}

max.loglike.MVNB <- function(parm){
  ...
  return(-loglike)
}
parm <- rep(0,9)
optim.MVNB <- optim(par = parm, max.loglike.MVNB)
```


```{r include = F, cache=T}

db.train <- df2 %>% filter(Type=='TRAIN')
db.test <- df2 %>% filter(Type=='TEST')

max.loglike.MVNB <- function(parm){
  beta <- parm[1:8]
  alpha <- 1/exp(parm[9])
  
  data$lambda <- data$risk_expo*exp(parm[1] + 
                                 parm[2]* (data$car_color=="Red") +
                                 parm[3]*(data$need_glasses=='Yes') +
                                 parm[4]*(data$territory=='Suburban') +
                                 parm[5]*(data$territory=='Urban') +
                                 parm[6]*(data$language=='French') +
                                 parm[7]*(data$food=='Vegan') +
                                 parm[8]*(data$food=='Vegetarian') )
  data <- data %>%
    group_by(policy_no, veh.num) %>%
    mutate(l.bullet = cumsum(lambda) - lambda,
           nb.bullet = cumsum(NbClaims) - NbClaims)%>%
    ungroup()
  
  data$tau.s = alpha + data$l.bullet
  data$alpha.s = alpha + data$nb.bullet

  ll <- lgamma(data$NbClaims + data$alpha.s) - lgamma(data$alpha.s) - lgamma(data$NbClaims+1) + 
    data$alpha.s*log(data$tau.s) - data$alpha.s*log(data$lambda+data$tau.s) +
    data$NbClaims*log(data$lambda) - data$NbClaims*log(data$lambda+data$tau.s)
  return(-sum(ll))
}

data <- db.train
parm <- c(Poisson$coefficients, -10)
max.loglike.MVNB(parm)
logLik(Poisson)

optim.MVNB <- optim(par = parm, max.loglike.MVNB)

```


---

# Results

## Comparison

```{r, echo=FALSE, warning=FALSE, message=FALSE}
parm <- Poisson$coefficients
MASS.parm <- c(nb2.MASS$coefficients, nb2.MASS$theta)
MVNB.parm <- c(optim.MVNB$par[1:8], exp(-optim.MVNB$par[9]))
parm[9] <- NA
table <- cbind(parm, MASS.parm, MVNB.parm)
colnames(table)[1] <- 'Poisson'
colnames(table)[2] <- 'NB2'
colnames(table)[3] <- 'MVNB'

rownames(table)[9] <- "$\\alpha$"
knitr::kable(table, format = "html", align = "ccccc",  digits = c(4,4,4), table.attr = "style='width:95%;'", format.args = list(big.mark = ",")) 

```

---

# Comparison of models

## Results

A correction for the training dataset, such as the AIC/BIC, must be applied for each model because they do not have he same number of parameters.

```{r, echo=FALSE, warning=FALSE, message=FALSE}

data <- db.test
parm <- optim.MVNB$par
logs.MVNB <-  max.loglike.MVNB(parm)

table<- data.frame(cbind(c(ll.Poisson,ll.NB2, -optim.MVNB$value), c(logs.Poisson,logs.NB2, logs.MVNB)))

colnames(table)[1] <- 'Log-likelihood (Train)'
colnames(table)[2] <- 'Logarithmic Score (Test)'
rownames(table)[1] <- 'Poisson'
rownames(table)[2] <- 'Negative Binomial 2'
rownames(table)[3] <- 'MVNB'

knitr::kable(table, format = "html", align = "ccccc",  digits = c(4,4), table.attr = "style='width:95%;'", format.args = list(big.mark = ",")) 

```

---

# Posterior ratemaking

As for cross-section data, we are interested to compute the predictive premiums. The predictive premium of an insured with is equal to:

$$E[N_{i,T}|n_{i,1},..., n_{i,T-1}, \boldsymbol{X}_{i,T}] = \lambda_{i,T} \frac{\alpha + \sum_{t=1}^{T-1} n_{i,t}}{\alpha + \sum_{t=1}^{T-1} \lambda_{i,t}}.$$

This result is similar to what we obtained for the predictive premium of a NB2 distribution but the estimated value of the 
$\alpha$ is not the same (estimates of $\beta$ are also different).

## Same problems 

For the same reasons why the NB2 was not used in practice (no weight in $\sum_{t=1}^{T-1} n_{i,t}$, and the need to use past $\lambda_{i,t}$, for example), another approach has to be used in practice.

As before, by knowing that the random effects $\Theta$ are gamma distributed, the Bonus-Malus Scale models can be an interesting solution...

---

# Predictive distribution

Even if the situation seems similar, cross-section data and panel data models are not the same. 

## Joint distribution

The joint distribution for all the contracts of the same insured can be rewritten as:

$$\Pr[N_{i,1}=n_{i,1},...,N_{i,T}=n_{i,T}] = \Pr[N_{i,1}=n_{i,1}] \times \Pr[N_{i,2}=n_{i,2}|n_{i,1}] \times \ldots \times \Pr[N_{i,T}=n_{i,T}|n_{i,1}, \ldots, n_{i,T-1}]$$

That means that the distribution the predictive distribution $\Pr[N_{i,t}=n_{i,t}|n_{i,1}, \ldots, n_{i,t-1}]$ is already used in the modeling and thus, a predictive premium is already computed in the underlying MVNB model.

## Assumption

For panel data, we do not need to rely on the assumption of the constant heterogeneity term to compute the predictive premium.

---

# Empirical analysis 

We can indeed verify empirically the values of the predictive premiums (as a percentage of the average frequency).

```{r, echo = F, warning=FALSE, dpi=300, fig.width=13, fig.height=5.5, fig.align = "center"}
mean <- mean(df2$NbClaims)
data <- df2 %>%
  arrange(policy_no, veh.num, renewal_date) %>%
  group_by(policy_no, veh.num) %>%
  mutate(contract.no = row_number(),
         past.nbclaim = cumsum(NbClaims)- NbClaims) %>%
  ungroup()

somm1 <- data %>%
  group_by(contract.no, past.nbclaim) %>%
  summarise(nb = n(),
            freq = mean(NbClaims), .groups = 'drop') %>%
  mutate(pct = freq/mean) %>%
  ungroup()

ggplot(somm1, aes(as.factor(contract.no), as.factor(past.nbclaim), fill= as.numeric(pct))) + 
  geom_tile()+
  geom_text(aes(label = paste(sprintf("%0.1f", 100*pct),'%  (', sprintf("%0.0f", nb), ')') ), color = "black", size = 4) +
  scale_fill_gradient2(low = "green", mid = "gray", midpoint = 1, high = "red")+
  xlab("Contract nb.") + ylab("Nb. of past claims")+
  theme(legend.position = 'none')
```

---

# Bonus-Malus scales

Despite everything, BMS are still interesting for actuaries and insurers:

- Advanced panel data models based on random effects, hierachical copulas, etc. cannot be easily used for ratemaking in practice; 

- The penalty structure of BMS are well-known by many insurers, brokers, regulators and insureds, and easy to explain/understand;

- BMS allow complex penalty structure that might be difficult to implement with classic statistical models:  
  + Fast-track for forgiveness (ex: for example: 3-5 years without claim automatically gives the largest discount);  
  + Multi-vehicules penalty structure;  
  + Multi-products penalty structure;  
  + etc.  

- There is a large scientific literature on BMS that can be used.

---

# The actual challenge with BMS

## Estimating the BMS 

The problem is not on the BMS itself, but on how BMS can now be estimated with the current longitudinal/hierarchical data of 
insurers:
  - We do not have to rely on the long-term behaviour of the insureds, based on the heterogeneity distribution;
  - BMS relativities have to be estimated directly with the data;
  - A direct comparison between BMS premiums and data can be done.

## Linking with basic GLM models

In the following slides, a link between the classic GLM approach and the BMS is proposed to better understand how actuaries can estimated all the BMS parameters.

---

# Using past informations as covariates 

Instead of using the bayesian approach, with an unknown risk profile that be updated after each contract, many insurers directly include past claims information as covariate in the $\mu$ mean parameter of the count distribution.  

For example, for an insured $i$ with $T-1$ years of experience, some actuaries use:
	
$$\mu_{i,T}= \exp\left(X_{i,T}' \beta + \gamma_{T-1} n_{i, T-1} + \gamma_2 n_{i, T-2} + ... +\gamma_1 n_{i, 1}\right),$$
where $\gamma_{1}, \ldots, \gamma_{T-1}$ can be estimated by ML, like the other $\beta$ parameters.

## Sum of past claims
	
This means that a large amount of parameters $\gamma_1, \ldots, \gamma_{T-1}$ must be estimated.  (One of) the purpose of statistic is to summarize information, so one classic approach is to use a summary of past claims.  For example, we can use:

$$\mu_{i,T}= \exp(X_{i,T}' \beta + \gamma n_{i, \bullet})$$

where $n_{i, \bullet} = \sum_{t=1}^{T-1} n_{i, t}$ is the number of all past claims for insured $i$. 

---

# New insureds and insureds with experience

The problem with the last approach is that it cannot differentiate new insureds from experience insureds without claim: both types of insureds have $n_{i, \bullet} = 0$. 

Instead, we should use:

$$\mu_{i,T}= \exp(\boldsymbol{X}_{i,T}' \beta + \gamma_1 \kappa_{i, \bullet} + \gamma_2 n_{i, \bullet})$$
	
where, for insured $i$:  
- $n_{i, \bullet} = \sum_{t=1}^{T-1} n_{i,t}$ is the number of all past claims;    
- $\kappa_{i, \bullet} = \sum_{t=1}^{T-1} I(n_{i,t}=0)$ is the sum of policy periods without claims.  

This allows us to differentiate new insureds from experienced insureds without claims.  
	
This model is called the **Kappa-N model**.

---

# Adding the past claim experience 

The information can be computed in the dataframe:

```{r, echo=FALSE, warning=FALSE}
knitr::kable(head(df2[,c(1:3, 23, 26:35)]), format = "html", table.attr = "style='width:95%;'") %>% scroll_box(width = "100%")
```

---

# Fitting the Kappa-N model 

The Kappa-N model only specifies the form of the mean and can be used with any distribution.  

The Poisson Kappa-N and the NB2 Kappa-N are presented below:

### Add past information

```{r, eval=TRUE}
data <- df2 %>%
  mutate(ind.0 = (NbClaims == 0)) %>%
  arrange(policy_no, veh.num, renewal_date) %>%
  group_by(policy_no, veh.num) %>%
  mutate(contract.no = row_number(),
         past.n = cumsum(NbClaims)- NbClaims, 
         past.kappa = cumsum(ind.0) - ind.0) %>%
  ungroup()
```

### Split data 

```{r, eval=TRUE}

db.train <- data %>% filter(Type=='TRAIN')
db.test <- data %>% filter(Type=='TEST')

```


---

# Poisson Kappa-N and NB2 Kappa-N models 

## Fitting models

```{r, eval=TRUE, cache=FALSE}
score.nbclaim <- as.formula(NbClaims ~ car_color + need_glasses + territory + language + food + past.n + past.kappa + offset(log(risk_expo)))
Poisson   <- glm(score.nbclaim, family=poisson(link=log), data=db.train)
nb2.MASS <- glm.nb(score.nbclaim, data=db.train) 
```

---

# Poisson Kappa-N and NB2 Kappa-N models 

## Estimated parameters 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
parm <- Poisson$coefficients
parm[11] <- NA
MASS.parm <- c(nb2.MASS$coefficients, nb2.MASS$theta)
table <- cbind(parm, MASS.parm)
colnames(table)[1] <- 'Poisson Kappa-N'
colnames(table)[2] <- 'NB2 Kappa-N'
rownames(table)[11] <- '$\\alpha$'
knitr::kable(table, format = "html", align = "ccccc",  digits = c(4,4), table.attr = "style='width:95%;'", format.args = list(big.mark = ",")) 
```

---

# Estimated parameters 

The mean parameter of the Poisson and the NB2 distribution was defined as:
	
$$\lambda_{i,t}= \exp(X_{i,t}' \beta + \gamma_1 \kappa_{i, \bullet} + \gamma_2 n_{i, \bullet})$$

## Results for the dataset of this presentation:

For the Poisson distribution, we obtained:

- $\widehat{\gamma}_1$ = $-0.1101$.
- $\widehat{\gamma}_2$ = $0.1976$;

## Results for the Farm dataset (in the published papers):

For the Poisson distribution, we obtained:

- $\widehat{\gamma}_1$ = $-0.0238$.  
- $\widehat{\gamma}_2$ = $0.0935$;   

---

# Rewriting the model

We can rewrite the Kappa-N model with the following steps:

1- Instead of using $\kappa_{i, \bullet}$ as a covariate, we used a slightly modified transformation: $100 - \kappa_{i, \bullet}$.  
  - The negative value in front of $\kappa_{i, \bullet}$ helps to understand that high values of contracts without claim should decrease the premium;
  - The value of $100$ will be used as the entry level for insureds without experience.

$$\lambda_{i,t} = \exp\left(\boldsymbol{X}_{i,t}' \beta^* + \gamma_1 (100 -\kappa_{i, \bullet}) + \gamma_2 n_{i, \bullet} \right)$$


2- We factor out the parameter $\gamma_1$ to obtain:

$$\lambda_{i,t} = \exp\left(\boldsymbol{X}_{i,t}' \beta^* + \gamma_1 \left(100 -\kappa_{i, \bullet} + \frac{\gamma_2}{\gamma_1} n_{i, \bullet} \right) \right) = \exp\left(\boldsymbol{X}_{i,t}' \beta^* + \gamma_1 \ell_{i,t} \right)$$

where the parameter $\ell_{i,t} = \left(100 -\kappa_{i, \bullet} + \frac{\gamma_2}{\gamma_1} n_{i, \bullet} \right)$ can be seen as a $\textbf{claim score}$.


---

# Penalty structure 

With the mean parameter:

$$\lambda_{i,t} = \exp\left(\boldsymbol{X}_{i,t}' \beta^* + \gamma_1 \ell_{i,t} \right) \text{, where } \ell_{i,t} = \left(100 -\kappa_{i, \bullet} + \frac{\gamma_2}{\gamma_1} n_{i, \bullet} \right)$$

## Impact of past claims

The Poisson Kappa-N model that we just estimated can then be analyzed as follow:

1) For new insured, without insured experience, we have $n_{i, \bullet} = 0$, and $\kappa_{i, \bullet} = 0$, which means an initial claim score $\ell_{i,1}$ of 100.  
  
2) Each year without claim decrease the claim score by 1.  

3) Each claim increases the claim score by $\Psi = \frac{\widehat{\gamma_2}}{\widehat{\gamma_1}} = \frac{0.1976}{0.1101} = 1.79$, called the *jump-parameter*:  
  - One claim equals $\approx \Psi$ years without claims.

---

# Penalty structure (2)

With the mean parameter:

$$\lambda_{i,t} = \exp\left(\boldsymbol{X}_{i,t}' \beta^* + \gamma_1 \ell_{i,t} \right) \text{, where } \ell_{i,t} = \left(100 -\kappa_{i, \bullet} + \frac{\gamma_2}{\gamma_1} n_{i, \bullet} \right)$$

## Impact of past claims

4) The penalty for a claim is equal to:
  - $\exp(\gamma_1 \times \Psi) - 1 = \exp(0.1101 \times 1.79) - 1  = 21.78\%$..

5) Each year without claim decreases the premium by:
  - $1 - \exp(-\gamma_1) = 1 - \exp(-0.1101) = 10.4\%$.	

---


# Problem of the Kappa-N models

One obvious problem with the Kappa-N models is the possible extreme values of $\ell_{i,t}$.  

For the fictive dataset, we have: 

1) Maximum value of $n_{i, \bullet}$: 8.    

2) Maximum value for $\ell_{i,t}$: 114.4     
  - Results in a premium almost 4 times higher than the premium for a new insured;  
  - It would take 14 consecutive years without claim for this insured to have the same premium as a new insured.

3) Minimum value for $\ell_{i,t}$: 96.  
  - Discount of $35\%$.  

---

# A possible solution

One solution could be to limit the value of $\ell_{i,t}$ in the modeling.  

For example, we can limit $\ell_{i,T}$ to be between 95 and 110, meaning $\ell_{min}=98$ and $\ell_{max}=105$:

- Instead of $\ell_{i,T} = 114.4$, an insured would have $\ell_{i,T}= 105$
  - ...but it would however still need him 14 consecutive years without claim to reach level 100!  

- Instead of $\ell_{i,T} = 96$, the insured without claim would have $\ell_{i,T}= 98$
  - ...but it means that he could claim without having any surcharge!  


---

# A better solution

Instead of:  
- limiting the claim score $\ell_{i,t}$ for **the current contract** $t=T$,   
- we could limit the value of the claim score $\ell_{i,t}$ but for **all past contracts** $t=1, \ldots, T$. 

## Simple illustration 

To better understand, we analyze three insureds for a model with a jump parameter $\Psi=3$. We want to estimate the premium of all three insureds at time $T=11$.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
s0 <- c(0,0,0,0,0,0,0,0,0,0)
s1 <- c(2,0,1,0,0,0,2,0,1,0)
s2 <- c(4,1,2,0,0,0,0,0,0,0)

table<- data.frame(rbind(s0, s1, s2)) 

colnames(table)[1] <- '1'
colnames(table)[2] <- '2'
colnames(table)[3] <- '3'
colnames(table)[4] <- '4'
colnames(table)[5] <- '5'
colnames(table)[6] <- '6'
colnames(table)[7] <- '7'
colnames(table)[8] <- '8'
colnames(table)[9] <- '9'
colnames(table)[10]<- '10'
rownames(table)[1] <- 'Insured 1'
rownames(table)[2] <- 'Insured 2'
rownames(table)[3] <- 'Insured 3'

knitr::kable(table, format = "html", align = "ccccc", table.attr = "style='width:95%;'", format.args = list(big.mark = ",")) %>%
  add_header_above(c("", "Number of claims at time (t)" = 10))

```


---



```{r, echo=FALSE, warning=FALSE, message=FALSE}

time <- seq(1:10)
s0 <- c(0,0,0,0,0,0,0,0,0,0)
s1 <- c(2,0,1,0,0,0,2,0,1,0)
s2 <- c(4,1,2,0,0,0,0,0,0,0)

table<- data.frame(cbind(time, s0, s1, s2)) 
colnames(table) <- c('time', 'nb1', 'nb2', 'nb3')
table$l1 <- NA
table$l2 <- NA
table$l3 <- NA

jump <- 3

lev1 <- 100
lev2 <- 100
lev3 <- 100
for(i in 1:10){
  table[i,5] <- lev1
  table[i,6] <- lev2
  table[i,7] <- lev3
  lev1 <- lev1 + jump*table[i,2] - (table[i,2]==0) 
  lev2 <- lev2 + jump*table[i,3] - (table[i,3]==0)
  lev3 <- lev3 + jump*table[i,4] - (table[i,4]==0)
}

g1<- ggplot() +
  geom_line(aes(x=time, y=l1), color='red', data=table)+
  geom_point(aes(x=time, y=l1), color='red', data=table)+
  geom_line(aes(x=time, y=l2), color='blue', data=table)+
  geom_point(aes(x=time, y=l2), color='blue', data=table)+ 
  geom_line(aes(x=time, y=l3), color='darkgreen', data=table)+
  geom_point(aes(x=time, y=l3), color='darkgreen', data=table)+
  scale_x_discrete(limits=1:10)+
  ylab("Claim Score") +
  xlab("Time") +
  ylim(90, 122)+
  theme_bw()


g2 <- ggplot() +
  geom_line(aes(x=time, y=l1), color='red', data=table)+
  geom_point(aes(x=time, y=l1), color='red', data=table)+
  geom_line(aes(x=time, y=l2), color='blue', data=table)+
  geom_point(aes(x=time, y=l2), color='blue', data=table)+ 
  geom_line(aes(x=time, y=l3), color='darkgreen', data=table)+
  geom_point(aes(x=time, y=l3), color='darkgreen', data=table)+
  geom_hline(yintercept=110, linetype="dashed", color = "black") +
  geom_hline(yintercept=95, linetype="dashed", color = "black") +
  geom_point(aes(x=10, y=110), color='blue', size=2, data=table)+
  geom_point(aes(x=10, y=110), color='darkgreen', size=2, data=table)+
  geom_point(aes(x=10, y=95), color='red', size=2, data=table)+
  scale_x_discrete(limits=1:10)+
  ylab("Claim Score") +
  xlab("Time") +
  ylim(90, 122)+
  theme_bw()


lev1 <- 100
lev2 <- 100
lev3 <- 100
for(i in 1:10){
  table[i,5] <- lev1
  table[i,6] <- lev2
  table[i,7] <- lev3
  lev1 <- lev1 + jump*table[i,2] - (table[i,2]==0) 
  lev2 <- lev2 + jump*table[i,3] - (table[i,3]==0)
  lev3 <- lev3 + jump*table[i,4] - (table[i,4]==0)
  lev1 <- min(max(lev1, 95), 110)
  lev2 <- min(max(lev2, 95), 110)
  lev3 <- min(max(lev3, 95), 110)
}

g3 <- ggplot() +
  geom_line(aes(x=time, y=l1), color='red', data=table)+
  geom_point(aes(x=time, y=l1), color='red', data=table)+
  geom_line(aes(x=time, y=l2), color='blue', data=table)+
  geom_point(aes(x=time, y=l2), color='blue', data=table)+ 
  geom_line(aes(x=time, y=l3), color='darkgreen', data=table)+
  geom_point(aes(x=time, y=l3), color='darkgreen', data=table)+
  geom_hline(yintercept=110, linetype="dashed", color = "black") +
  geom_hline(yintercept=95, linetype="dashed", color = "black") +
  scale_x_discrete(limits=1:10)+
  ylab("Claim Score") +
  xlab("Time") +
  ylim(90, 122)+
  theme_bw()

g3b <- ggplot() +
  geom_line(aes(x=time, y=l1), color='red', data=table)+
  geom_point(aes(x=time, y=l1), color='red', data=table)+
  geom_line(aes(x=time, y=l2), color='blue', data=table)+
  geom_point(aes(x=time, y=l2), color='blue', data=table)+ 
  geom_line(aes(x=time, y=l3), color='darkgreen', data=table)+
  geom_point(aes(x=time, y=l3), color='darkgreen', data=table)+
  geom_hline(yintercept=110, linetype="dashed", color = "black") +
  geom_hline(yintercept=95, linetype="dashed", color = "black") +
  scale_x_discrete(limits=1:10)+
  ylab("Claim Score") +
  xlab("Time") +
  ylim(90, 115)+
  theme_bw()

jump <- 4
lev1 <- 100
lev2 <- 100
lev3 <- 100
for(i in 1:10){
  table[i,5] <- lev1
  table[i,6] <- lev2
  table[i,7] <- lev3
  lev1 <- lev1 + jump*table[i,2] - (table[i,2]==0) 
  lev2 <- lev2 + jump*table[i,3] - (table[i,3]==0)
  lev3 <- lev3 + jump*table[i,4] - (table[i,4]==0)
  lev1 <- min(max(lev1, 92), 112)
  lev2 <- min(max(lev2, 92), 112)
  lev3 <- min(max(lev3, 92), 112)
}

g4 <- ggplot() +
  geom_line(aes(x=time, y=l1), color='red', data=table)+
  geom_point(aes(x=time, y=l1), color='red', data=table)+
  geom_line(aes(x=time, y=l2), color='blue', data=table)+
  geom_point(aes(x=time, y=l2), color='blue', data=table)+ 
  geom_line(aes(x=time, y=l3), color='darkgreen', data=table)+
  geom_point(aes(x=time, y=l3), color='darkgreen', data=table)+
  geom_point(aes(x=10, y=95), color='red', size=2, shape=15)+
  geom_point(aes(x=10, y=110), color='blue', size=2, shape=15)+
  geom_point(aes(x=10, y=104), color='darkgreen', size=2, shape=15)+
  geom_hline(yintercept=112, linetype="dashed", color = "black") +
  geom_hline(yintercept=92, linetype="dashed", color = "black") +
  scale_x_discrete(limits=1:10)+
  ylab("Claim Score") +
  xlab("Time") +
  ylim(90, 115)+
  theme_bw()



 
```

# How to limit the claim score

```{r, echo = F, warning=FALSE, dpi=300, fig.width=13, fig.height=5.5, fig.align = "center"}

g1

```


---

# How to limit the claim score

```{r, echo = F, warning=FALSE, dpi=300, fig.width=13, fig.height=5.5, fig.align = "center"}

g2

```

---

# How to limit the claim score

```{r, echo = F, warning=FALSE, dpi=300, fig.width=13, fig.height=5.5, fig.align = "center"}

g3

```

---

# Kappa-N model becomes a BMS model

By limiting the claim score for all past contracts, the Kappa-N model becomes a Bonus-Malus Scale Model. The claim score of insured $i$ at time $T$, $\ell_{i,T}$, can now be seen as a BMS level. 

A BMS without limits $\ell_{min} \rightarrow -\infty$ and $\ell_{max} \rightarrow \infty$ is a Kappa-N model.

## Joint Distribution of all contracts

The joint distribution can now be expressed as the product of simple count distributions (with mean that depends on the Bonus-Malus level):

$$\Pr[N_{i,1}=n_{i,1},...,N_{i,T}=n_{i,T}] = \Pr[N_{i,1}=n_{i,1}|\ell_{i,1}] \times \Pr[N_{i,2}=n_{i,2}|\ell_{i,2}] \times \ldots \times \Pr[N_{i,T}=n_{i,T}|\ell_{i,T}]$$
where the markovian property of the BMS level can be used with:

$$\ell_{i,t} = \min(\max(\ell_{i,t-1} - I(n_{i,t-1}=0) + \Psi \times n_{i,t-1}, \ell_{min}), \ell_{max})$$
---

# Impact of the structural parameters of the BMS

To summarize, the BMS level $\ell_{i,t}$ depends on:  

1- The jump parameter $\Psi$;  
2- The minimum limit $\ell_{min}$;  
3- The maximum limit $\ell_{max}$.    

Changing one of these three structural parameters will also changes the value of $\ell_{i,t}$, which means that the mean parameter of the count distribution of $N_{i,t}$ will change.

## BMS level path

It is important to understand that for each combinaison of the structural parameters $\Psi, \ell_{min}$ and $\ell_{max}$, the whole experience of each insured must be recomputed to obtain the correct BMS levels $\ell_{i,t}$.

---

# Impact: example

## Choice of structural parameters 

With $\Psi = 3, \ell_{min} = 95, \ell_{max} =110$:

```{r, echo = F, warning=FALSE, dpi=300, fig.width=13, fig.height=4.5, fig.align = "center"}

g3b

```

---

# Impact: example

## Choice of structural parameters 

With $\Psi = 4, \ell_{min} = 92, \ell_{max} =112$:

```{r, echo = F, warning=FALSE, dpi=300, fig.width=13, fig.height=4.5, fig.align = "center"}

g4

```

---

# Parameters inference 

## Regresion parameters

For any insurance database, when the structural parameters are set, we can now compute the Bonus-Malus level of all contracts of each insured.  We then apply a simple regression model with mean:  

$$\lambda_{i,t} = \exp\left(X_{i,t}' \beta^* + \gamma_1 \ell_{i,t} \right),$$
and estimate the parameters $\beta$, $\gamma_1$ and other parameters from the distribution (a dispersion parameter for example). The classic *GLM* package can be used for a Poisson, and the *MASS* package for the negative binomial.

## Structural parameters

When the structural parameters $\Psi, \ell_{min}$ and $\ell_{max}$ are selected, it is easy to estimate the parameters of the BMS model.

But finding the best values of $\Psi, \ell_{min}$ and $\ell_{max}$ cannot be done directly. Even if we limit the strutural parameters to be integer, computing all possibilities might be too long.

---

# For small dataset

For a small dataset such as the one used in this presentation, we can simply test all possibles values of the structural parameters.

```{r, eval=FALSE}
Psi     <- seq(1, 10, length.out = 10)
ell.min <- seq(96, 99, length.out = 4)
ell.max <- seq(101, 120, length.out = 20)
grid <- expand.grid(ell.max = ell.max, ell.min = ell.min, Psi = Psi)
grid$llPoisson <- NA
grid$llNB2 <- NA

for(ii in 1:nrow(grid)){
  data         <- set.BMS_levels(ell.max=grid[ii,1], ell.min=grid[ii,2], Psi=grid[ii,3], db.train)
  PoissonBMS   <- glm(score.nbclaim, family=poisson(link=log), data=data)
  nb2BMS       <- glm.nb(score.nbclaim, data=data)
  grid[ii,4] <- logLik(PoissonBMS)
  grid[ii,5] <- logLik(nb2BMS)
}
print(grid[grid$llPoisson==max(grid$llPoisson),])
print(grid[grid$llNB2==max(grid$llNB2),])

```

The best BMS model, for the Poisson and the NB2 distributions, is $\ell_{max}=104, \ell_{max}=96, \Psi=2$.

---

# Proposed algorithm 

For real insurance data, testing all possibilities is too long. 
A proposed iterative technique based on profile log-likelihood works as follow:

**Initial step:**

We set $\ell_{min}^{(0)} \rightarrow -\infty$ and $\ell_{max}^{(0)} \rightarrow \infty$ (this represents the Kappa-N model). We can then directly estimate a first estimate of the jump $\Psi^{(0)} = \Psi$. 

**For step $k=1,...$:**  

- With $\Psi = \Psi^{(k-1)}$ and $\ell_{min} = \ell_{min}^{(k-1)}$, we estimate all possible BMS models for any value of $\ell_{max}$.    
  - We choose $\ell_{max}^{(k)} = \ell_{max}$ from the best BMS model.  
- With $\Psi = \Psi^{(k-1)}$ and $\ell_{max} = \ell_{max}^{(k)}$, we estimate all possible BMS models for any value of $\ell_{min}$.  
  - We choose $\ell_{min}^{(k)} = \ell_{min}$ from the best BMS model.  
- With $\ell_{max} = \ell_{max}^{(k)}$ and $\ell_{min} = \ell_{min}^{(k)}$, we estimate all possible BMS models for any value of $\Psi$.       
  - We choose $\Psi^{(k)} = \Psi$ from the best BMS model.  

We repeat these steps until we reach convergence. 

---

# Fitting the BMS (Poisson distribution)

## Estimated parameters

```{r, eval=TRUE, cache=TRUE, echo=FALSE}
score.nbclaim <- as.formula(NbClaims ~ car_color + need_glasses + territory + language + food + offset(log(risk_expo)))
Poisson   <- glm(score.nbclaim, family=poisson(link=log), data=db.train)
NB2       <- glm.nb(score.nbclaim, data=db.train) 

score.nbclaim <- as.formula(NbClaims ~ car_color + need_glasses + territory + language + food + past.n + past.k + offset(log(risk_expo)))
Poisson.KN   <- glm(score.nbclaim, family=poisson(link=log), data=db.train)
NB2.KN <- glm.nb(score.nbclaim, data=db.train) 

set.BMS_levels <- function(ell.max, ell.min, Psi, df2){
  tempo1 <- df2[which(df2$contract.number == 1),]
  tempo1$BMS.level <- 100
  tempo2 <- df2[which(df2$contract.number == 2),]
  tempo2$BMS.level <- pmin(ell.max, pmax(ell.min, 100 - tempo2$lagk1 + Psi*tempo2$lagn1))
  tempo3 <- df2[which(df2$contract.number == 3),]
  tempo3$BMS.level <- pmin(ell.max, pmax(ell.min, 100 - tempo3$lagk2 + Psi*tempo3$lagn2))
  tempo3$BMS.level <- pmin(ell.max, pmax(ell.min, tempo3$BMS.level - tempo3$lagk1 + Psi*tempo3$lagn1))
  tempo4 <- df2[which(df2$contract.number == 4),]
  tempo4$BMS.level <- pmin(ell.max, pmax(ell.min, 100 - tempo4$lagk3 + Psi*tempo4$lagn3))
  tempo4$BMS.level <- pmin(ell.max, pmax(ell.min, tempo4$BMS.level - tempo4$lagk2 + Psi*tempo4$lagn2))
  tempo4$BMS.level <- pmin(ell.max, pmax(ell.min, tempo4$BMS.level - tempo4$lagk1 + Psi*tempo4$lagn1))
  tempo5 <- df2[which(df2$contract.number == 5),]
  tempo5$BMS.level <- pmin(ell.max, pmax(ell.min, 100 - tempo5$lagk4 + Psi*tempo5$lagn4))
  tempo5$BMS.level <- pmin(ell.max, pmax(ell.min, tempo5$BMS.level - tempo5$lagk3 + Psi*tempo5$lagn3))
  tempo5$BMS.level <- pmin(ell.max, pmax(ell.min, tempo5$BMS.level - tempo5$lagk2 + Psi*tempo5$lagn2))
  tempo5$BMS.level <- pmin(ell.max, pmax(ell.min, tempo5$BMS.level - tempo5$lagk1 + Psi*tempo5$lagn1))
  data <- rbind(tempo1, tempo2, tempo3, tempo4, tempo5)
  data <- data %>%
    arrange(policy_no, veh.num, renewal_date)
  return(data)
}

data <- set.BMS_levels(ell.max=104, ell.min=96, Psi=2, df2)
db.train <- data %>% filter(Type=='TRAIN')
db.test <- data %>% filter(Type=='TEST')

score.nbclaim <- as.formula(NbClaims ~ car_color + need_glasses + territory + language + food + BMS.level + offset(log(risk_expo)))
Poisson.BMS   <- glm(score.nbclaim, family=poisson(link=log), data=db.train)
NB2.BMS <- glm.nb(score.nbclaim, data=db.train) 

```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
parm <- c(Poisson$coefficients, NA, NA)
parm.nb2 <- c(NB2$coefficients, NB2$theta)

parm.KN <- Poisson.KN$coefficients
parm.nb2.KN <- c(NB2$coefficients, NB2$theta)

parm.BMS <- c(Poisson.BMS$coefficients, NA)
parm.nb2.BMS <- c(NB2.BMS$coefficients, NB2.BMS$theta)

table <- cbind(parm, parm.KN, parm.BMS)
colnames(table)[1] <- 'Poisson'
colnames(table)[2] <- 'Poisson Kappa-N'
colnames(table)[3] <- 'Poisson BMS (104/96/+2)'
rownames(table)[9] <- '$\\gamma_1$'
rownames(table)[10] <- '$\\gamma_2$'
knitr::kable(table, format = "html", align = "ccccc",  digits = c(4,4), table.attr = "style='width:95%;'", format.args = list(big.mark = ",")) 
```

---

# Analyzing the model 

## Expected number of claims

The mean of the BMS model can then be expressed as:

\begin{align*}
\lambda_{i,t} =& \exp\left(X_{i,t}' \beta^* + \gamma_1 \ell_{i,t} \right) 
= \exp\left(X_{i,t}' \beta^* + \gamma_1 \ell_{i,t} \right) 
= \Pi_{i,t} \times r(\ell_{i,t})
\end{align*}

where: 
- $\Pi_{i,t} = \exp\left(X_{i,t}' \beta^*\right)$ is the base premium that depends on covariates for the contract $t$ of insured $i$;
- $r(\ell_{i,t}) = \exp\left(\gamma_1 \ell_{i,t} \right)$ is the BMS relativity that depends on the BMS level $\ell_{i,t}$ the contract $t$ of insured $i$.

## Prior and posterior ratemaking

As opposed to classic BMS calibration, all parameters of the mean are estimated simulatenously:

i) the $\beta$ from the covariates (for the $\Pi_{i,t}$ component),  
ii) the $\gamma_1$ for the predictive ratemaking (for the $r(\ell_{i,t})$ component).  


---

# Graph of BMS relativities 


```{r, echo = F, warning=FALSE, dpi=300, fig.width=13, fig.height=5.5, fig.align = "center"}

BMS.s    <- 104
BMS.min  <- 96
BMS.ell0 <- 100
BMS.psi  <- 2

min1 <- BMS.min
max1 <- BMS.s

#coef <- as.numeric(Poisson.BMS$coefficients[9])

coef <- 0.1180

nb <- floor(max1-min1 + 1)
rel.mat <- matrix(ncol=2, nrow=nb)
for(i in 1:nb){
  BMS.level <- as.numeric(BMS.min - 1 + i)
  rel.BMS <- exp((BMS.level-100)*coef)
  rel.mat[i, 1] <- BMS.level
  rel.mat[i, 2] <- rel.BMS
}
rel.mat.SI <- data.frame(rel.mat)
colnames(rel.mat.SI) <- c("BMS.level", "rel")

## calculs pour le graph ###
floor_dec <- function(x, level=1) round(x - 5*10^(-level-1), level)
ceiling_dec <- function(x, level=1) round(x + 5*10^(-level-1), level)

y.max <- max(rel.mat.SI$rel)
y.maxc <- ceiling_dec(y.max, 1)
y.min <- min(rel.mat.SI$rel)
y.minf <- floor_dec(y.min, 1)

x.loc.psi <- BMS.psi/2 + 100
y.loc.psi <- y.min + 0.06
y.psi <- exp(BMS.psi*coef)
y.kappa <- exp(-coef)
up <- 0.02


ggplot(rel.mat.SI) +
  geom_line(aes(x=BMS.level, y = rel, colour="Relativity")) + 
  ## maximum 
  #  geom_segment(x = BMS.s, xend=BMS.s, y = 0, yend=y.max, colour = "black", size = 0.3) +
  geom_segment(x = 0, xend=BMS.s, y = y.max , yend=y.max, colour = "blue", linetype="dashed",  size = 0.3) +
  ## minimum 
  #  geom_segment(x = BMS.min, xend=BMS.min, y = 0, yend=y.min, colour = "black", size = 0.3) +
  geom_segment(x = 0, xend=BMS.min, y = y.min , yend=y.min, colour = "blue", linetype="dashed", size = 0.3) +
  ## relativite 1.00
  geom_segment(x = 100, xend=100, y = 0, yend=1, colour = "black", linetype="dashed",size = 0.3) +
  geom_segment(x = 0, xend=100, y = 1, yend=1, colour = "black", linetype="dashed",size = 0.3) +
  geom_point(aes(x=100, y=1), colour="black") + 
  ## Claim + Psi
  annotate("text", x = x.loc.psi, y = y.loc.psi, label = '+Psi', color="red", parse = TRUE, size=5) +
  geom_segment(aes(x = 100, y = y.min + up, xend = 100+BMS.psi, yend = y.min + up), color="red", arrow = arrow(length = unit(0.5, "cm"))) +
  geom_segment(x = 100 + BMS.psi, xend=100+ BMS.psi, y = 0, yend=y.psi, colour = "red", linetype="dashed",size = 0.3) +
  geom_segment(x = 0, xend=100 + BMS.psi, y = y.psi, yend=y.psi, colour = "red", linetype="dashed",size = 0.3) +
  geom_point(aes(x=100+BMS.psi, y=y.psi), colour="red") + 
  ## No Claim - 1
  annotate("text", x = 99.4, y = y.loc.psi, label = '-1', color="darkgreen", parse = TRUE, size=5) +
  geom_segment(aes(x = 100, y = y.min + up, xend = 99, yend = y.min + up), color="darkgreen", arrow = arrow(length = unit(0.5, "cm"))) +
  geom_segment(x = 99, xend=99, y = 0, yend=y.kappa, colour = "darkgreen", linetype="dashed",size = 0.3) +
  geom_segment(x = 0, xend=99, y = y.kappa, yend=y.kappa, colour = "darkgreen", linetype="dashed",size = 0.3) +
  geom_point(aes(x=99, y=y.kappa), colour="darkgreen") + 
  
  scale_colour_manual("", values = c("BMS Relativity"="blue"))+
  ylab("Relativity") + xlab("BMS Level") +
  scale_x_continuous(breaks=c(seq(BMS.min,BMS.s,by=1), BMS.s)) +
  scale_y_continuous(breaks=c(seq(y.minf,y.maxc,by=0.1))) +
  theme(legend.position="none") +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black"),
        text = element_text(size = 15))


```


---

# Fitting and predictive quality

### Log-likelihood (Training set) 

A correction, such as the AIC/BIC, must be applied for each model because they do not have he same number of parameters.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
ll.Poisson    = logLik(Poisson)
ll.Poisson.KN  = logLik(Poisson.KN)
ll.Poisson.BMS = logLik(Poisson.BMS)

ll.NB2    = logLik(NB2)
ll.NB2.KN  = logLik(NB2.KN)
ll.NB2.BMS = logLik(NB2.BMS)

ll.MVNB <- -optim.MVNB$value
  
table<- data.frame(cbind(c(ll.Poisson,ll.NB2, ll.MVNB), c(ll.Poisson.KN,ll.NB2.KN, NA), c(ll.Poisson.BMS,ll.NB2.BMS, NA)))

colnames(table)[1] <- 'Standard'
colnames(table)[2] <- 'Kappa-N'
colnames(table)[3] <- 'BMS'
rownames(table)[1] <- 'Poisson'
rownames(table)[2] <- 'Negative Binomial 2'
rownames(table)[3] <- 'MVNB'

knitr::kable(table, format = "html", align = "ccccc",  digits = c(4,4), table.attr = "style='width:95%;'", format.args = list(big.mark = ",")) 

```

### Logarithmic Score (Test set)

```{r, echo=FALSE, warning=FALSE, message=FALSE}
data <- set.BMS_levels(ell.max=104, ell.min=96, Psi=2, df2)
db.train <- data %>% filter(Type=='TRAIN')
db.test <- data %>% filter(Type=='TEST')

db.test$pred <- predict(Poisson, newdata=db.test, type="response") 
test.Poisson <- -sum(dpois(db.test$NbClaims, db.test$pred, log=TRUE))
db.test$pred <- predict(NB2, newdata=db.test, type="response") 
alpha <- 1/NB2$theta
tau <- 1/NB2$theta
ll <- lgamma(db.test$NbClaims + alpha) - lgamma(alpha) - lgamma(db.test$NbClaims+1) + alpha*log(tau) - alpha*log(db.test$pred+tau) + db.test$NbClaims*log(db.test$pred) - db.test$NbClaims*log(db.test$pred+tau)
test.NB2 <- -sum(ll)

db.test$pred <- predict(Poisson.KN, newdata=db.test, type="response") 
test.Poisson.KN <- -sum(dpois(db.test$NbClaims, db.test$pred, log=TRUE))
db.test$pred <- predict(NB2.KN, newdata=db.test, type="response") 
alpha <- 1/NB2.KN$theta
tau <- 1/NB2.KN$theta
ll <- lgamma(db.test$NbClaims + alpha) - lgamma(alpha) - lgamma(db.test$NbClaims+1) + alpha*log(tau) - alpha*log(db.test$pred+tau) + db.test$NbClaims*log(db.test$pred) - db.test$NbClaims*log(db.test$pred+tau)
test.NB2.KN <- -sum(ll)

db.test$pred <- predict(Poisson.BMS, newdata=db.test, type="response") 
test.Poisson.BMS <- -sum(dpois(db.test$NbClaims, db.test$pred, log=TRUE))
db.test$pred <- predict(NB2.BMS, newdata=db.test, type="response") 
alpha <- 1/NB2.BMS$theta
tau <- 1/NB2.BMS$theta
ll <- lgamma(db.test$NbClaims + alpha) - lgamma(alpha) - lgamma(db.test$NbClaims+1) + alpha*log(tau) - alpha*log(db.test$pred+tau) + db.test$NbClaims*log(db.test$pred) - db.test$NbClaims*log(db.test$pred+tau)
test.NB2.BMS <- -sum(ll)

```

```{r, echo=FALSE, warning=FALSE, message=FALSE}

table<- data.frame(cbind(c(test.Poisson,test.NB2, logs.MVNB), c(test.Poisson.KN,test.NB2.KN, NA), c(test.Poisson.BMS,test.NB2.BMS, NA)))

colnames(table)[1] <- 'Standard'
colnames(table)[2] <- 'Kappa-N'
colnames(table)[3] <- 'BMS'
rownames(table)[1] <- 'Poisson'
rownames(table)[2] <- 'Negative Binomial 2'
rownames(table)[3] <- 'MVNB'

knitr::kable(table, format = "html", align = "ccccc",  digits = c(4,4), table.attr = "style='width:95%;'", format.args = list(big.mark = ",")) 

```

---

# Distribution over the BMS levels (training dataset)

```{r, echo=FALSE, warning=FALSE, message=FALSE}
data <- set.BMS_levels(ell.max=104, ell.min=96, Psi=2, df2)
db.train <- data %>% filter(Type=='TRAIN')
db.test <- data %>% filter(Type=='TEST')

db.train$pred <- predict(Poisson.BMS, newdata=db.train, type="response") 
db.test$pred <- predict(Poisson.BMS, newdata=db.test, type="response") 

train <- db.train %>%
  group_by(BMS.level) %>%
  summarize(tot.claims = mean(NbClaims),
            tot.pred = mean(pred)) 

test <- db.test %>%
  group_by(BMS.level) %>%
  summarize(tot.claims = mean(NbClaims),
            tot.pred = mean(pred)) 

gr1 <- ggplot(data=train) + 
  geom_line(aes(x=BMS.level, y=tot.claims, color="Observed")) +
  geom_point(aes(x=BMS.level, y=tot.claims, color="Observed")) +
  geom_line(aes(x=BMS.level, y=tot.pred, color="Fitted")) +
  geom_point(aes(x=BMS.level, y=tot.pred, color="Fitted")) +
  ylab("Average Frequency") + xlab("BMS Level") +
  scale_colour_manual("", 
                      breaks = c("Observed", "Fitted"),
                      values = c("red", "blue")) +
  theme_bw()

gr2 <- ggplot(data=test) + 
  geom_line(aes(x=BMS.level, y=tot.claims, color="Observed")) +
  geom_point(aes(x=BMS.level, y=tot.claims, color="Observed")) +
  geom_line(aes(x=BMS.level, y=tot.pred, color="Predicted")) +
  geom_point(aes(x=BMS.level, y=tot.pred, color="Predicted")) +
  ylab("Average Frequency") + xlab("BMS Level") +
  scale_colour_manual("", 
                      breaks = c("Observed", "Predicted"),
                      values = c("red", "blue")) +
  theme_bw()


```


```{r, echo = F, warning=FALSE, dpi=300, fig.width=13, fig.height=5.5, fig.align = "center"}

gr1

```

---

# Distribution over the BMS levels (test dataset)

```{r, echo = F, warning=FALSE, dpi=300, fig.width=13, fig.height=5.5, fig.align = "center"}

gr2

```

---

class: inverse

## Part I - Ratemaking with Cross-Section Data     

- Basic count distributions  
- Credibility Models and Predictive Ratemaking  
- Bonus-Malus Scales Models  

## Part II - Ratemaking with Panel Data  

- Families of Count Distributions  
- Observed Predictive Premiums  
- Bonus-Malus Scales Models Revisited

## .rougeb[Part III - Actual Challenges]

- Entry levels and new insureds  
- Penalties and *a priori* risks

---

background-image: url("images/Paper1.png")
background-position: 80% 50%
background-size: 40%

# Reference 

.pull-left[
### This part of the presentation is based on Section 4 of: 

J.-P. Boucher (2022). Bonus-Malus Scale Models: Creating Artificial Past Claims History. *Annals of Actuarial Science*, 1-27. ]

---

# Joint distribution

We already mentioned that the joint distribution of all claim counts of each insured $i=1,\ldots,m$ can be expressed as:

$$\Pr[N_{i,1}=n_{i,1},...,N_{i,T}=n_{i,T}] = \Pr[N_{i,1}=n_{i,1}|\ell_{i,1}] \times \Pr[N_{i,2}=n_{i,2}|\ell_{i,2}] \times \ldots \times \Pr[N_{i,T}=n_{i,T}|\ell_{i,T}]$$
where the markovian property of the BMS level can be used with:

$$\ell_{i,t} = \min(\max(\ell_{i,t-1} - I(n_{i,t-1}=0) + \Psi \times n_{i,t-1}, \ell_{min}), \ell_{max})$$
## New insureds

We have a problem with $\Pr[N_{i,1}=n_{i,1}|\ell_{i,1}]$: thos insureds at time $t=1$ are not always new drivers, but often new insureds in the company or new insured in the database. For the first contract at $t=1$, we do not have $\ell_{i,0}$ nor $n_{i,0}$. 

The major problem with past claims rating refers to the availability of past information: 

- Insurers are not able to obtain past information from other insurers;
- Insurers are also often unable to use information from their own old contracts (modification of their operating systems, when past databases are simply erased or are no longer useful, etc.).

---

# Timeline

The figure bellow illustrates the situation, where the timeline is divided in two sections: 

i) The Past Claims Information section: the time period where past claims information is available to compute $n_{i, \bullet}$ and $\kappa_{i, \bullet}$, or the Bonus-Malus level.   
ii) The Artificial Information section, from the date of the first insurance contract of insured $i$) to $\tau_i^{(1)}$.  

When the available past claims information is short, experience rating models might be difficult to estimate because the amount of information needed to compute the bonus-malus level, for example, is too small.  

```{r, echo = F, out.width = "70%", fig.align = "center"}
knitr::include_graphics("images/PastInfo.png")
```

---

# Artificial claim history

In the actuarial literature, two methods have been proposed to generate an artificial past claims history:

### 1- Average outcome

The first method is to suppose that all unobserved years of experience of insured $i$ involve an average expected number of claims $\tilde{\mu}_i$.  

This implies a corrected version of $n_{i, \bullet}$ and $\kappa_{i, \bullet}$ for each insured, and consequently a new value of $\ell_{i,t}$ for all $t$.

### 2- Most probable outcome

The second method is to suppose that each unobserved year of experience would be considered a year without claims, simply because is the most probable outcome. 

This assumption simplifies greatly the computation and the estimation of the BMS because it means using the first method with $\tilde{\mu}_i = 0$ for all insureds.  



---

background-image: url("images/Paper2.png")
background-position: 80% 50%
background-size: 40%

# Reference

.pull-left[
### This part of the presentation is based on: 

J.-P. Boucher (2022). Multiple Bonus-Malus Scale Models for Insureds of Different Sizes. *Risks*, 10(8), 152. ]

---

# Size of each farm

Remember the predictive expected premium for the Poisson-gamma model:

$$E[N_{i,T}|n_{i,1},..., n_{i,T-1}, \boldsymbol{X}_{i,T}] = \lambda_{i,T} \frac{\alpha + \sum_{t=1}^{T-1} n_{i,t}}{\alpha + \sum_{t=1}^{T-1} \lambda_{i,t}}$$


or the weights in the Buhlmann-Straub model, where the random variables were normalised by the *a priori* risk: $Y_{i,t} = \frac{N_{i,t}}{W_{i,t}}$.

That means that the experience of an insured is *normalized* when it is used in predictive ratemaking.  BMS models does not do that: the penalty for a claim does not depend on the *a priori* risk.

## Farm insurance 

This caused a problem in farm insurance where large farms can be penalized twice:
- In their *a priori* risk;  
- With the BMS structure (because they claim more).

---

# Bonus-Malus vs. size of the farm

```{r, echo = F, out.width = "70%", fig.align = "center"}
knitr::include_graphics("images/MeanItem.png")
```

---

# Bonus-Malus vs. size of the farm

```{r, echo = F, out.width = "70%", fig.align = "center"}
knitr::include_graphics("images/MeanLevel.png")
```

---

# Partitioning the portfolio

A proposed solution to deal with farms of different sizes was to divide the portfolio into groups.  

Groups of farms of similar sizes could be created, and each group would have their own experience-rating model, with its own *a priori* rating parameters and its own structural BMS parameters. 

Farms could then be more equitably rated, and more correctly rewarded and penalized, as their size would be directly taken into account when performing past claims rating.

## Recursive algorithm

To find the best way to group similar farms, a recursive algorithm was proposed.

---

# Partitioning the portfolio by the size

```{r, echo = F, out.width = "70%", fig.align = "center"}
knitr::include_graphics("images/partition.png")
```

---


# Conclusion

- Bonus-Malus Scales models are already in use in many countries;  

- Because of how insurance datasets are now constructed, the current way to calibrate BMS has to be changed;  

- We show that iterative GLM approaches can be used to estimate BMS models.  

- BMS are really flexible and allows penalty structures that cannot be supposed easily by other models and distributions;  

- Despite its simplicity, it has been shown to out-perform many more *advanced* models;  

---

background-image: url("images/Chaire.png")
background-position: 80% 50%
background-size: 40%

#  Website of the research Chair 

.pull-left[
You can check the <a href="https://chairecara.uqam.ca/en/">website</a> of the *Co-operators Chair in Actuarial Risk Analysis* (CARA) for:
- recent publications, 
- research projects;
- MSc and PhD fundings, 
- etc.:

<center><a href="https://chairecara.uqam.ca/en/">https://chairecara.uqam.ca/en/</a></center>
]

---

background-image: url("images/GitHub.png")
background-position: 80% 50%
background-size: 40%

#  Github 

.pull-left[

On my <a href="https://github.com/J-PBoucher/CAS_SanDiego2023">github page</a>, you can find:
- this presentation; 
- all R script codes used;
- the dataframe *df2.Rda*.

<center><a href="https://github.com/J-PBoucher">https://github.com/J-PBoucher</a></center>
]

---

# Other

## References

- J.-P. Boucher (2022). Bonus-Malus Scale Models: Creating Artificial Past Claims History. *Annals of Actuarial Science*, 1-27. 
- J.-P. Boucher (2022). Multiple Bonus-Malus Scale Models for Insureds of Different Sizes. *Risks*, 10(8), 152. 
- J.-P. Boucher & M. Pigeon (2019), A Claim Score for Dynamic Claim Counts Modelling, *Canadian Institute of Actuaries*.
- J.-P. Boucher & R. Inoussa (2014). A posteriori ratemaking with panel data. *ASTIN Bulletin: The Journal of the IAA*, 44(3), 587-612.

## Thanks

Finally, a special thank to my Ph.D. student **Francis Duval** who created this nice *xaringan* template with RMarkdown.


